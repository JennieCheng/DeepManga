{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datetime import time\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "lr = 3E-4\n",
    "momentum = 0.9\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "cuda = False\n",
    "\n",
    "log_interval = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def gs_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "\n",
    "            return img.convert('L')\n",
    "\n",
    "def load_data(parent_dir,transform = transforms.ToTensor()):\n",
    "    return dset.ImageFolder(root=parent_dir, transform = transform, loader=gs_loader)\n",
    "\n",
    "\n",
    "def load_target_mapping(filename = \"mapping.json\"):\n",
    "    with open(filename) as data_file:\n",
    "        data = json.load(data_file)\n",
    "        #collapse authors\n",
    "        author = {}\n",
    "        authors = []\n",
    "        invertedMap = {}\n",
    "        for object in data:\n",
    "            a_name = object[\"Author\"]\n",
    "            f_name = object[\"Folder Name\"]\n",
    "            vol_name = object[\"Volume in dataset\"]\n",
    "            invertedMap[f_name] = a_name\n",
    "            if vol_name is not None:\n",
    "                invertedMap[f_name+\"_vol{0:02d}\".format(vol_name)] = a_name\n",
    "            if not author.has_key(a_name) :\n",
    "                author[a_name] = []\n",
    "                authors.append(a_name)\n",
    "            author[a_name].append(f_name)\n",
    "            invertedMap[f_name] = a_name\n",
    "\n",
    "        return  author, invertedMap, authors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    load_data(\"../../Data/Manga109_processed/images\"),\n",
    "    batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class convclassifier(nn.Module):\n",
    "    def __init__(self, input_shape=(1, 256, 256),out_targets = 109):\n",
    "        super(convclassifier, self).__init__()\n",
    "\n",
    "\n",
    "        #encoder part\n",
    "        self.en_conv_1 = nn.Conv2d(1, 100, 5)\n",
    "        self.en_max_pool_1 =  nn.MaxPool2d(2,return_indices=False)  # b, 16, 5, 5\n",
    "        self.en_conv_2 = nn.Conv2d(100, 200, 5) # b, 8, 3, 3\n",
    "        self.en_max_pool_2 = nn.MaxPool2d(2,return_indices=False)  # b, 8, 2, 2\n",
    "\n",
    "        input_size = self._get_conv_output_size(input_shape)\n",
    "        self.dense_400 = nn.Linear(input_size,400)\n",
    "        self.dense_200 = nn.Linear(400, 200)\n",
    "        self.dense_out = nn.Linear(200, out_targets)\n",
    "\n",
    "    def load_my_state_dict(self, state_dict):\n",
    "\n",
    "        own_state = self.state_dict()\n",
    "        for name, param in state_dict.items():\n",
    "            if name not in own_state:\n",
    "                continue\n",
    "            if isinstance(param, Parameter):\n",
    "                # backwards compatibility for serialized parameters\n",
    "                param = param.data\n",
    "            own_state[name].copy_(param)\n",
    "    def _get_conv_output_size(self, shape):\n",
    "        bs = 32\n",
    "        input = Variable(torch.rand(bs, *shape))\n",
    "        output_feat = self.en_conv_1(input)\n",
    "        output_feat = self.en_max_pool_1(output_feat)\n",
    "        output_feat = self.en_conv_2(output_feat)\n",
    "        output_feat = self.en_max_pool_2(output_feat)\n",
    "        n_size = output_feat.data.view(bs, -1).size(1)\n",
    "        return n_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.en_conv_1(x)\n",
    "        x = self.en_max_pool_1(x)\n",
    "        x = self.en_conv_2(x)\n",
    "        x = self.en_max_pool_2(x)\n",
    "        Generate_images\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.dense_400(x)\n",
    "        x = self.dense_200(x)\n",
    "        x = self.dense_out(x)\n",
    "\n",
    "\n",
    "        return F.log_softmax(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
